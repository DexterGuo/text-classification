{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import ndarray as nd\n",
    "from data_helper.mr_loader import *\n",
    "from mxnet import gluon, autograd, io\n",
    "import mxnet as mx\n",
    "from mxnet.gluon.data import Dataset, DataLoader\n",
    "\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu()\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = 'data/mr'\n",
    "pos_file = os.path.join(base_dir, 'rt-polarity.pos.txt')\n",
    "neg_file = os.path.join(base_dir, 'rt-polarity.neg.txt')\n",
    "vocab_file = os.path.join(base_dir, 'rt-polarity.vocab.txt')\n",
    "\n",
    "\n",
    "save_path = 'checkpoints'  # model save path\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "model_file = os.path.join(save_path, 'mr_cnn.params')\n",
    "\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"\n",
    "    Return the time used since start_time.\n",
    "    \"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conv_Max_Pooling(nn.Block):\n",
    "    def __init__(self, channels, kernel_size, **kwargs):\n",
    "        super(Conv_Max_Pooling, self).__init__(**kwargs)\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.conv = nn.Conv1D(channels, kernel_size)\n",
    "            self.pooling = nn.GlobalMaxPool1D()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.pooling(self.conv(x))\n",
    "        return nd.relu(output).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"\n",
    "    CNN parameters\n",
    "    \"\"\"\n",
    "    embedding_dim = 128  # embedding vector size\n",
    "    seq_length = 50  # maximum length of sequence\n",
    "    vocab_size = 8000  # most common words\n",
    "\n",
    "    num_filters = 100  # number of the convolution filters (feature maps)\n",
    "    kernel_sizes = [3, 4, 5]   # three kinds of kernels (windows)\n",
    "\n",
    "    dropout_prob = 0.5  # dropout rate\n",
    "    learning_rate = 1e-3  # learning rate\n",
    "    batch_size = 50  # batch size for training\n",
    "    num_epochs = 20  # total number of epochs\n",
    "\n",
    "    num_classes = 2  # number of classes\n",
    "\n",
    "    dev_split = 0.1  # percentage of dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Block):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        \n",
    "        V = config.vocab_size\n",
    "        E = config.embedding_dim\n",
    "        Nf = config.num_filters\n",
    "        Ks = config.kernel_sizes\n",
    "        C = config.num_classes\n",
    "        Dr = config.dropout_prob\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(V, E)\n",
    "            self.conv1 = Conv_Max_Pooling(Nf, Ks[0])\n",
    "            self.conv2 = Conv_Max_Pooling(Nf, Ks[1])\n",
    "            self.conv3 = Conv_Max_Pooling(Nf, Ks[2])\n",
    "            self.dropout = nn.Dropout(Dr)\n",
    "            self.fc1 = nn.Dense(C)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose((0, 2, 1))\n",
    "        o1, o2, o3 = self.conv1(x), self.conv2(x), self.conv3(x)\n",
    "        outputs = self.fc1(self.dropout(nd.concat(o1, o2, o3)))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MRDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(MRDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index].astype(np.float32), self.y[index].astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(data_iterator, net, loss, ctx):\n",
    "    \"\"\"\n",
    "    Evaluation, return accuracy and loss\n",
    "    \"\"\"\n",
    "    total_loss, data_len = 0.0, 0\n",
    "    acc = mx.metric.Accuracy() \n",
    "    \n",
    "    for data, label in data_iterator:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "        \n",
    "        with autograd.record(train_mode=False):\n",
    "            output = net(data)\n",
    "            losses = loss(output, label)\n",
    "        \n",
    "        total_loss += nd.sum(losses).asscalar()\n",
    "        data_len += len(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1], total_loss / data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "start_time = time.time()\n",
    "config = Config()\n",
    "corpus = Corpus(pos_file, neg_file, vocab_file, config.dev_split, config.seq_length, config.vocab_size)\n",
    "print(corpus)\n",
    "config.vocab_size = len(corpus.words)\n",
    "\n",
    "print(\"Configuring CNN model...\")\n",
    "ctx = try_gpu()\n",
    "model = TextCNN(config)\n",
    "model.collect_params().initialize(ctx=ctx)\n",
    "print(\"Initializing weights on\", ctx)\n",
    "print(model)\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': config.learning_rate})\n",
    "\n",
    "train_loader = DataLoader(MRDataset(corpus.x_train, corpus.y_train), batch_size=config.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(MRDataset(corpus.x_test, corpus.y_test), batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "print(\"Training and evaluating...\")\n",
    "best_acc = 0.0\n",
    "for epoch in range(config.num_epochs):\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "        batch_len = len(data)\n",
    "        \n",
    "        with autograd.record(train_mode=True):\n",
    "            output = model(data)\n",
    "            losses = loss(output, label)\n",
    "            \n",
    "        losses.backward()\n",
    "        trainer.step(batch_len)\n",
    "    \n",
    "    train_acc, train_loss = evaluate(train_loader, model, loss, ctx)\n",
    "    test_acc, test_loss = evaluate(test_loader, model, loss, ctx)\n",
    "    \n",
    "    if test_acc > best_acc:\n",
    "        # store the best validation result\n",
    "        best_acc = test_acc\n",
    "        improved_str = '*'\n",
    "        model.save_params(model_file)\n",
    "    else:\n",
    "        improved_str = ''\n",
    "    \n",
    "    time_dif = get_time_dif(start_time)\n",
    "    msg = \"Epoch {0:3}, Train_loss: {1:>7.2}, Train_acc {2:>6.2%}, \" \\\n",
    "        + \"Test_loss: {3:>6.2}, Test_acc {4:>6.2%}, Time: {5} {6}\"\n",
    "    print(msg.format(epoch + 1, train_loss, train_acc, test_loss, test_acc, time_dif, improved_str))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    Train and evaluate the model with training and validation data.\n",
    "    \"\"\"\n",
    "    print('Loading data...')\n",
    "    start_time = time.time()\n",
    "    config = Config()\n",
    "    corpus = Corpus(pos_file, neg_file, vocab_file, config.dev_split, config.seq_length, config.vocab_size)\n",
    "    print(corpus)\n",
    "    config.vocab_size = len(corpus.words)\n",
    "\n",
    "    print('Configuring CNN model...')\n",
    "    model = CNN(config)\n",
    "\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(model.collect_params(), 'sgd', {'learning_rate': 0.01})\n",
    "    \n",
    "    total_batch = 0\n",
    "    total_loss = 0.0\n",
    "    best_acc_val = 0.0\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        # load the training data in batch\n",
    "        train_loader = io.NDArrayIter(data={'data': nd.array(corpus.x_train)}, \n",
    "                                      label={'label': nd.array(corpus.y_train)}, \n",
    "                                      batch_size=config.batch_size)\n",
    "        for batch in train_loader:\n",
    "            cur_batch = config.batch_size - batch.pad\n",
    "            with autograd.record():\n",
    "                output = model(batch.data[0])\n",
    "                loss = softmax_cross_entropy(output, batch.label[:cur_batch])\n",
    "            loss.backward()\n",
    "            trainer.step(cur_batch)\n",
    "            print(loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # forward computation\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.data[0]\n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # print out intermediate status\n",
    "                avg_loss = total_loss / config.print_per_batch\n",
    "                total_loss = 0.0\n",
    "\n",
    "                _, pred_train = torch.max(outputs.data, 1)\n",
    "                corrects = (pred_train == targets.data).sum()\n",
    "                acc_train = corrects / len(x_batch)\n",
    "                loss_val, acc_val, _ = evaluate(model, val_data)  # evaluate on val data\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                    # store the best validation result\n",
    "                    best_acc_val = acc_val\n",
    "                    improved_str = '*'\n",
    "                    torch.save(model.state_dict(), model_file)\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, avg_loss, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "            # back propagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    test(model, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
