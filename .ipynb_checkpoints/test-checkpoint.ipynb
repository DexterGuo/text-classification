{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd, metric\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "from data_helper.mr_loader import Corpus, read_vocab, process_text\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data/mr'\n",
    "pos_file = os.path.join(base_dir, 'rt-polarity.pos.txt')\n",
    "neg_file = os.path.join(base_dir, 'rt-polarity.neg.txt')\n",
    "vocab_file = os.path.join(base_dir, 'rt-polarity.vocab.txt')\n",
    "\n",
    "save_path = 'checkpoints'  # model save path\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "model_file = os.path.join(save_path, 'mr_cnn.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu()\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"\n",
    "    CNN parameters\n",
    "    \"\"\"\n",
    "    embedding_dim = 128  # embedding vector size\n",
    "    seq_length = 50  # maximum length of sequence\n",
    "    vocab_size = 8000  # most common words\n",
    "\n",
    "    num_filters = 100  # number of the convolution filters (feature maps)\n",
    "    kernel_sizes = [3, 4, 5]   # three kinds of kernels (windows)\n",
    "\n",
    "    dropout_prob = 0.5  # dropout rate\n",
    "    learning_rate = 1e-3  # learning rate\n",
    "    batch_size = 50  # batch size for training\n",
    "    num_epochs = 3  # total number of epochs\n",
    "\n",
    "    num_classes = 2  # number of classes\n",
    "\n",
    "    dev_split = 0.1  # percentage of dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TCNNConfig(object):\n",
    "    \"\"\"\n",
    "    CNN parameters\n",
    "    \"\"\"\n",
    "    embedding_dim = 128  # embedding vector size\n",
    "    seq_length = 50  # maximum length of sequence\n",
    "    vocab_size = 8000  # most common words\n",
    "\n",
    "    num_filters = 100  # number of the convolution filters (feature maps)\n",
    "    kernel_sizes = [3, 4, 5]   # three kinds of kernels (windows)\n",
    "\n",
    "    dropout_prob = 0.5  # dropout rate\n",
    "    learning_rate = 1e-3  # learning rate\n",
    "    batch_size = 50  # batch size for training\n",
    "    num_epochs = 20  # total number of epochs\n",
    "\n",
    "    num_classes = 2  # number of classes\n",
    "\n",
    "    dev_split = 0.1  # percentage of dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conv_Max_Pooling(nn.Block):\n",
    "    \"\"\"\n",
    "    Integration of Conv1D and GlobalMaxPool1D layers\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, kernel_size, **kwargs):\n",
    "        super(Conv_Max_Pooling, self).__init__(**kwargs)\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.conv = nn.Conv1D(channels, kernel_size)\n",
    "            self.pooling = nn.GlobalMaxPool1D()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pooling(self.conv(x))\n",
    "        return nd.relu(output).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Block):\n",
    "    \"\"\"\n",
    "    CNN text classification model, based on the paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "\n",
    "        V = config.vocab_size\n",
    "        E = config.embedding_dim\n",
    "        Nf = config.num_filters\n",
    "        Ks = config.kernel_sizes\n",
    "        C = config.num_classes\n",
    "        Dr = config.dropout_prob\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(V, E)  # embedding layer\n",
    "\n",
    "            # three different convolutional layers\n",
    "            self.conv1 = Conv_Max_Pooling(Nf, Ks[0])\n",
    "            self.conv2 = Conv_Max_Pooling(Nf, Ks[1])\n",
    "            self.conv3 = Conv_Max_Pooling(Nf, Ks[2])\n",
    "            self.dropout = nn.Dropout(Dr)  # a dropout layer\n",
    "            self.fc1 = nn.Dense(C)         # a dense layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose((0, 2, 1))   # Conv1D takes in NCW as input\n",
    "        o1, o2, o3 = self.conv1(x), self.conv2(x), self.conv3(x)\n",
    "        outputs = self.fc1(self.dropout(nd.concat(o1, o2, o3)))\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_dif(start_time):\n",
    "    \"\"\"\n",
    "    Return the time used since start_time.\n",
    "    \"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MRDataset(Dataset):\n",
    "    \"\"\"\n",
    "    An implementation of the Abstracted gluon.data.Dataset, used for loading data in batch\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        super(MRDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index].astype(np.float32), self.y[index].astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate2(data_iterator, net, loss, ctx):\n",
    "    \"\"\"\n",
    "    Evaluation, return accuracy and loss\n",
    "    \"\"\"\n",
    "    total_loss, data_len = 0.0, 0\n",
    "    acc = mx.metric.Accuracy() \n",
    "    \n",
    "    for data, label in data_iterator:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "        \n",
    "        with autograd.record(train_mode=False):\n",
    "            output = net(data)\n",
    "            losses = loss(output, label)\n",
    "        \n",
    "        total_loss += nd.sum(losses).asscalar()\n",
    "        data_len += len(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1], total_loss / data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(data_iterator, data_len, net, loss, ctx):\n",
    "    \"\"\"\n",
    "    Evaluation, return accuracy and loss\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    acc = metric.Accuracy()\n",
    "\n",
    "    for data, label in data_iterator:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "\n",
    "        with autograd.record(train_mode=False): # set the training_mode to False\n",
    "            output = net(data)\n",
    "            losses = loss(output, label)\n",
    "\n",
    "        total_loss += nd.sum(losses).asscalar()\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    print('data_len', data_len)\n",
    "    print('acc', acc.get()[1])\n",
    "    print('total_loss', total_loss / data_len)\n",
    "    return acc.get()[1], total_loss / data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    Train and evaluate the model with training and test data.\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    start_time = time.time()\n",
    "    config = Config()\n",
    "    corpus = Corpus(pos_file, neg_file, vocab_file, config.dev_split, config.seq_length, config.vocab_size)\n",
    "    print(corpus)\n",
    "    config.vocab_size = len(corpus.words)\n",
    "\n",
    "    print(\"Configuring CNN model...\")\n",
    "    ctx = try_gpu()\n",
    "    model = TextCNN(config)\n",
    "    model.collect_params().initialize(ctx=ctx)\n",
    "    print(\"Initializing weights on\", ctx)\n",
    "    print(model)\n",
    "\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': config.learning_rate})\n",
    "\n",
    "    batch_size = config.batch_size\n",
    "    train_loader = DataLoader(MRDataset(corpus.x_train, corpus.y_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(MRDataset(corpus.x_test, corpus.y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"Training and evaluating...\")\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(config.num_epochs):\n",
    "        for data, label in train_loader:\n",
    "            data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "\n",
    "            with autograd.record(train_mode=True):   # set the model in training mode\n",
    "                output = model(data)\n",
    "                losses = loss(output, label)\n",
    "\n",
    "            # backward propagation and update parameters\n",
    "            losses.backward()\n",
    "            trainer.step(len(data))\n",
    "\n",
    "        # evaluate on both training and test dataset\n",
    "        train_acc, train_loss = evaluate(train_loader, len(corpus.x_train), model, loss, ctx)\n",
    "        test_acc, test_loss = evaluate(test_loader, len(corpus.x_test), model, loss, ctx)\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            # store the best result\n",
    "            best_acc = test_acc\n",
    "            improved_str = '*'\n",
    "            model.save_params(model_file)\n",
    "        else:\n",
    "            improved_str = ''\n",
    "\n",
    "        time_dif = get_time_dif(start_time)\n",
    "        msg = \"Epoch {0:3}, Train_loss: {1:>7.2}, Train_acc {2:>6.2%}, \" \\\n",
    "              + \"Test_loss: {3:>6.2}, Test_acc {4:>6.2%}, Time: {5} {6}\"\n",
    "        print(type(train_loss))\n",
    "        print(train_loss)\n",
    "        print(type(train_acc))\n",
    "        print(type(test_loss))\n",
    "        print(test_loss)\n",
    "        print(type(test_acc))\n",
    "\n",
    "        print(msg.format(epoch + 1, train_loss, train_acc, test_loss, test_acc, time_dif, improved_str))\n",
    "\n",
    "    test(model, test_loader, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, ctx):\n",
    "    \"\"\"\n",
    "    Test the model on test dataset.\n",
    "    \"\"\"\n",
    "    print(\"Testing...\")\n",
    "    start_time = time.time()\n",
    "    model.load_params(model_file, ctx=ctx)\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for data, label in test_loader:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "        with autograd.record(train_mode=False): # set the training_mode to False\n",
    "            output = model(data)\n",
    "        pred = nd.argmax(output, axis=1).asnumpy().tolist()\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(label.asnumpy().tolist())\n",
    "\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(metrics.classification_report(y_true, y_pred, target_names=['POS', 'NEG']))\n",
    "\n",
    "    print('Confusion Matrix...')\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    print(\"Time usage:\", get_time_dif(start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    # load config and vocabulary\n",
    "    config = Config()\n",
    "    _, word_to_id = read_vocab(vocab_file)\n",
    "    labels = ['POS', 'NEG']\n",
    "\n",
    "    # load model\n",
    "    ctx = try_gpu()\n",
    "    model = TextCNN(config)\n",
    "    model.load_params(model_file, ctx=ctx)\n",
    "\n",
    "    # process text\n",
    "    text = process_text(text, word_to_id, config.seq_length)\n",
    "    text = nd.array([text]).as_in_context(ctx)\n",
    "\n",
    "    output = model(text)\n",
    "    pred = nd.argmax(output, axis=1).asscalar()\n",
    "\n",
    "    return labels[int(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training: 9595, Testing: 1067, Vocabulary: 8000\n",
      "Configuring CNN model...\n",
      "Initializing weights on gpu(0)\n",
      "TextCNN(\n",
      "  (embedding): Embedding(8000 -> 128, float32)\n",
      "  (conv1): Conv_Max_Pooling(\n",
      "    (conv): Conv1D(None -> 100, kernel_size=(3,), stride=(1,))\n",
      "    (pooling): GlobalMaxPool1D(size=(1,), stride=(1,), padding=(0,), ceil_mode=True)\n",
      "  )\n",
      "  (conv2): Conv_Max_Pooling(\n",
      "    (conv): Conv1D(None -> 100, kernel_size=(4,), stride=(1,))\n",
      "    (pooling): GlobalMaxPool1D(size=(1,), stride=(1,), padding=(0,), ceil_mode=True)\n",
      "  )\n",
      "  (conv3): Conv_Max_Pooling(\n",
      "    (conv): Conv1D(None -> 100, kernel_size=(5,), stride=(1,))\n",
      "    (pooling): GlobalMaxPool1D(size=(1,), stride=(1,), padding=(0,), ceil_mode=True)\n",
      "  )\n",
      "  (dropout): Dropout(p = 0.5)\n",
      "  (fc1): Dense(None -> 2, linear)\n",
      ")\n",
      "Training and evaluating...\n",
      "<class 'numpy.float64'>\n",
      "0.427711034877\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "0.519968409123\n",
      "<class 'numpy.float64'>\n",
      "Epoch   1, Train_loss:    0.43, Train_acc 82.68%, Test_loss:   0.52, Test_acc 74.41%, Time: 0:00:28 *\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
