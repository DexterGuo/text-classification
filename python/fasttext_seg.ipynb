{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import preprocessing as ps\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_range = 2\n",
    "max_features = 100000\n",
    "batch_size = 32\n",
    "embedding_dims = 128\n",
    "epochs = 10\n",
    "maxlen = 1000\n",
    "index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train, label_train = ps.read_data_maxlen('../train.txt', index, maxlen)\n",
    "data_test, label_test = ps.read_data_maxlen('../test.txt', index, maxlen)\n",
    "data_val, label_val = ps.read_data_maxlen('../val.txt', index, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seg_all(data):\n",
    "    return [ps.segment(x) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train = seg_all(data_train)\n",
    "data_test = seg_all(data_test)\n",
    "data_val = seg_all(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_file(data, label, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for i in range(len(data)):\n",
    "            f.write(' '.join(data[i]) + '\\t' + label[i] + '\\n')\n",
    "\n",
    "def load_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = []\n",
    "        label = []\n",
    "        for line in f:\n",
    "            data.append(line.split('\\t')[0].split(' '))\n",
    "            label.append(line.split('\\t')[1])\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_to_file(data_train, label_train, '../train_seg.txt')\n",
    "save_to_file(data_test, label_test, '../test_seg.txt')\n",
    "save_to_file(data_val, label_val, '../val_seg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train, label_train = load_file('../train_seg.txt')\n",
    "data_test, label_test = load_file('../test_seg.txt')\n",
    "data_val, label_val = load_file('../val_seg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(max(map(len, data_train)))\n",
    "print(max(map(len, data_test)))\n",
    "print(max(map(len, data_val)))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.mean(list(map(len, data_train))))\n",
    "print(np.mean(list(map(len, data_test))))\n",
    "print(np.mean(list(map(len, data_val))))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words(data, max_features):\n",
    "    text = []\n",
    "    for line in data:\n",
    "        text = text + line\n",
    "    counter = Counter(text).most_common(max_features)\n",
    "    words, _ = zip(*counter)\n",
    "    \n",
    "    word_to_id = dict((c, i) for i, c in enumerate(words))\n",
    "    id_to_word = dict((i, c) for i, c in enumerate(words))\n",
    "    return words, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_line(data, max_features):\n",
    "    print(len(data))\n",
    "    counter = Counter()\n",
    "    cnt = 0\n",
    "    for line in data:\n",
    "        counter = counter + Counter(line)\n",
    "        \n",
    "        cnt += 1\n",
    "        if cnt % 50 == 0:\n",
    "            print(cnt, len(counter))\n",
    "    \n",
    "    counter = counter.most_common(max_features)\n",
    "    words, _ = zip(*counter)\n",
    "    \n",
    "    word_to_id = dict((c, i) for i, c in enumerate(words))\n",
    "    id_to_word = dict((i, c) for i, c in enumerate(words))\n",
    "    return words, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words_batch(data, max_features, batch_size):\n",
    "    counter = Counter()\n",
    "    batch_num = len(data) // batch_size\n",
    "    \n",
    "    for i in range(batch_num-1):\n",
    "        text = []\n",
    "        for line in data[i*batch_size:(i+1)*batch_size]:\n",
    "            text += line\n",
    "        counter += Counter(text)\n",
    "    \n",
    "    text = []\n",
    "    for line in data[batch_size*(batch_num-1):]:\n",
    "        text += line\n",
    "    counter += Counter(text)\n",
    "    \n",
    "    counter = counter.most_common(max_features)\n",
    "    words, _ = zip(*counter)\n",
    "    \n",
    "    word_to_id = dict((c, i) for i, c in enumerate(words))\n",
    "    id_to_word = dict((i, c) for i, c in enumerate(words))\n",
    "\n",
    "    return words, word_to_id, id_to_word, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words, word_to_id, id_to_word, counter = get_words_batch(data_train + data_test + data_val, max_features, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_set, cls_to_id, id_to_cls = ps.get_classes(label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = ps.tokenize(data_train, label_train, word_to_id, cls_to_id, len(class_set))\n",
    "X_test, y_test = ps.tokenize(data_test, label_test, word_to_id, cls_to_id, len(class_set))\n",
    "X_val, y_val = ps.tokenize(data_val, label_val, word_to_id, cls_to_id, len(class_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(max(map(len, X_train)))\n",
    "print(max(map(len, X_test)))\n",
    "print(max(map(len, X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_indice, max_features = ps.build_ngram_tokens(X_train, max_features, ngram_range)\n",
    "X_train = ps.pad_ngram_data(X_train, token_indice, maxlen, ngram_range)\n",
    "X_test = ps.pad_ngram_data(X_test, token_indice, maxlen, ngram_range)\n",
    "X_val = ps.pad_ngram_data(X_val, token_indice, maxlen, ngram_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = ps.data_shuffle(X_train, y_train)\n",
    "X_test, Y_test = ps.data_shuffle(X_test, y_test)\n",
    "X_val, Y_val = ps.data_shuffle(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "# 先从一个高效的嵌入层开始，它将词汇表索引映射到 embedding_dim 维度的向量上\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "# we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "# of all words in the document\n",
    "# 添加一个 GlobalAveragePooling1D 层，它将平均整个序列的词嵌入\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "# 投影到一个单神经元输出层，然后使用 sigmoid 挤压。\n",
    "model.add(Dense(len(class_set), activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()  # 概述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
