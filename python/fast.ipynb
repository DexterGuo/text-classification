{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import preprocessing as ps\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "titles_train, labels_train = ps.read_data('../train.txt')\n",
    "titles_test, labels_test = ps.read_data('../test.txt')\n",
    "titles_val, labels_val = ps.read_data('../val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words, word_to_id, id_to_word = ps.get_words(titles_train + titles_test + titles_val)\n",
    "class_set, cls_to_id, id_to_cls = ps.get_classes(labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = ps.tokenize(titles_train, labels_train, word_to_id, cls_to_id, len(class_set))\n",
    "X_test, y_test = ps.tokenize(titles_test, labels_test, word_to_id, cls_to_id, len(class_set))\n",
    "X_val, y_val = ps.tokenize(titles_val, labels_val, word_to_id, cls_to_id, len(class_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ngram_range = 2\n",
    "max_features = len(words)\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "maxlen = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 2-gram features\n"
     ]
    }
   ],
   "source": [
    "token_indice, max_features = ps.build_ngram_tokens(X_train, max_features, ngram_range)\n",
    "X_train = ps.pad_ngram_data(X_train, token_indice, maxlen, ngram_range)\n",
    "X_test = ps.pad_ngram_data(X_test, token_indice, maxlen, ngram_range)\n",
    "X_val = ps.pad_ngram_data(X_val, token_indice, maxlen, ngram_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 50)            11429500  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 11,430,010.0\n",
      "Trainable params: 11,430,010.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "# 先从一个高效的嵌入层开始，它将词汇表索引映射到 embedding_dim 维度的向量上\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "# we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "# of all words in the document\n",
    "# 添加一个 GlobalAveragePooling1D 层，它将平均整个序列的词嵌入\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "# 投影到一个单神经元输出层，然后使用 sigmoid 挤压。\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()  # 概述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "29s - loss: 1.3605 - acc: 0.6837 - val_loss: 0.7641 - val_acc: 0.7914\n",
      "Epoch 2/10\n",
      "28s - loss: 0.4595 - acc: 0.8816 - val_loss: 0.5645 - val_acc: 0.8336\n",
      "Epoch 3/10\n",
      "27s - loss: 0.2401 - acc: 0.9387 - val_loss: 0.5085 - val_acc: 0.8482\n",
      "Epoch 4/10\n",
      "28s - loss: 0.1303 - acc: 0.9699 - val_loss: 0.4962 - val_acc: 0.8506\n",
      "Epoch 5/10\n",
      "28s - loss: 0.0701 - acc: 0.9853 - val_loss: 0.5108 - val_acc: 0.8518\n",
      "Epoch 6/10\n",
      "28s - loss: 0.0390 - acc: 0.9923 - val_loss: 0.5329 - val_acc: 0.8530\n",
      "Epoch 7/10\n",
      "28s - loss: 0.0234 - acc: 0.9950 - val_loss: 0.5558 - val_acc: 0.8530\n",
      "Epoch 8/10\n",
      "28s - loss: 0.0155 - acc: 0.9961 - val_loss: 0.5842 - val_acc: 0.8492\n",
      "Epoch 9/10\n",
      "27s - loss: 0.0117 - acc: 0.9966 - val_loss: 0.6154 - val_acc: 0.8492\n",
      "Epoch 10/10\n",
      "28s - loss: 0.0097 - acc: 0.9967 - val_loss: 0.6437 - val_acc: 0.8474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a11dae6e48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练与验证\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_val, y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.642158113983\n",
      "Test accuracy: 0.8408\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=2)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
