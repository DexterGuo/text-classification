{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import ndarray as nd\n",
    "from data_helper.mr_loader import *\n",
    "from mxnet import gluon, autograd, io\n",
    "import mxnet as mx\n",
    "\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = 'data/mr'\n",
    "pos_file = os.path.join(base_dir, 'rt-polarity.pos.txt')\n",
    "neg_file = os.path.join(base_dir, 'rt-polarity.neg.txt')\n",
    "vocab_file = os.path.join(base_dir, 'rt-polarity.vocab.txt')\n",
    "\n",
    "\n",
    "save_path = 'save_models'  # model save path\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "model_file = os.path.join(save_path, 'mr_cnn.pt')\n",
    "\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"\n",
    "    Return the time used since start_time.\n",
    "    \"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conv_Max_Pooling(nn.Block):\n",
    "    def __init__(self, channels, kernel_size, **kwargs):\n",
    "        super(Conv_Max_Pooling, self).__init__(**kwargs)\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.conv = nn.Conv1D(channels, kernel_size)\n",
    "            self.pooling = nn.GlobalMaxPool1D()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.pooling(self.conv(x))\n",
    "        return nd.relu(output).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"\n",
    "    CNN parameters\n",
    "    \"\"\"\n",
    "    embedding_dim = 128  # embedding vector size\n",
    "    seq_length = 50  # maximum length of sequence\n",
    "    vocab_size = 8000  # most common words\n",
    "\n",
    "    num_filters = 100  # number of the convolution filters (feature maps)\n",
    "    kernel_sizes = [3, 4, 5]   # three kind of kernels (windows)\n",
    "\n",
    "    hidden_dim = 128  # hidden size of fully connected layer\n",
    "\n",
    "    dropout_prob = 0.5  # how much probability to be dropped\n",
    "    learning_rate = 1e-3  # learning rate\n",
    "    batch_size = 50  # batch size for training\n",
    "    num_epochs = 20  # total number of epochs\n",
    "\n",
    "    print_per_batch = 50 # print out the intermediate status every n batches\n",
    "\n",
    "    num_classes = 2  # number of classes\n",
    "\n",
    "    dev_split = 0.1  # percentage of dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Block):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(CNN, self).__init__(**kwargs)\n",
    "        \n",
    "        V = config.vocab_size\n",
    "        E = config.embedding_dim\n",
    "        Nf = config.num_filters\n",
    "        Ks = config.kernel_sizes\n",
    "        C = config.num_classes\n",
    "        dropout = config.dropout_prob\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(V, E)\n",
    "            self.conv1 = Conv_Max_Pooling(Nf, Ks[0])\n",
    "            self.conv2 = Conv_Max_Pooling(Nf, Ks[1])\n",
    "            self.conv3 = Conv_Max_Pooling(Nf, Ks[2])\n",
    "            self.fc1 = nn.Dense(C)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose((0, 2, 1))\n",
    "        o1, o2, o3 = self.conv1(x), self.conv2(x), self.conv3(x)\n",
    "        outputs = self.fc1(nd.concat(o1, o2, o3))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MRDataset(gluon.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(MRDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index].astype(np.float32), self.y[index].astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training: 9595, Testing: 1067, Vocabulary: 8000\n",
      "Configuring CNN model...\n",
      "Epoch: 1\n",
      "Epoch 0. Loss: 0.659942268344, Train_acc 0.767170401251, Test_acc 0.696344892221\n",
      "Epoch: 2\n",
      "Epoch 1. Loss: 0.42460086737, Train_acc 0.922876498176, Test_acc 0.765698219306\n",
      "Epoch: 3\n",
      "Epoch 2. Loss: 0.228351220195, Train_acc 0.968108389786, Test_acc 0.746016869728\n",
      "Epoch: 4\n",
      "Epoch 3. Loss: 0.111777372296, Train_acc 0.985617509119, Test_acc 0.741330834114\n",
      "Epoch: 5\n",
      "Epoch 4. Loss: 0.0505577494416, Train_acc 0.990724335591, Test_acc 0.749765698219\n",
      "Epoch: 6\n",
      "Epoch 5. Loss: 0.0258927302537, Train_acc 0.998332464825, Test_acc 0.759137769447\n",
      "Epoch: 7\n",
      "Epoch 6. Loss: 0.0225548458571, Train_acc 0.994267847837, Test_acc 0.761012183693\n",
      "Epoch: 8\n",
      "Epoch 7. Loss: 0.0257302293372, Train_acc 0.998228243877, Test_acc 0.746954076851\n",
      "Epoch: 9\n",
      "Epoch 8. Loss: 0.0203728131853, Train_acc 0.997498697238, Test_acc 0.733833177132\n",
      "Epoch: 10\n",
      "Epoch 9. Loss: 0.0109604583976, Train_acc 0.999687337155, Test_acc 0.751640112465\n",
      "Epoch: 11\n",
      "Epoch 10. Loss: 0.00367081132545, Train_acc 1.0, Test_acc 0.751640112465\n",
      "Epoch: 12\n",
      "Epoch 11. Loss: 0.00160322880888, Train_acc 1.0, Test_acc 0.748828491097\n",
      "Epoch: 13\n",
      "Epoch 12. Loss: 0.000970798582926, Train_acc 1.0, Test_acc 0.750702905342\n",
      "Epoch: 14\n",
      "Epoch 13. Loss: 0.000807877033056, Train_acc 1.0, Test_acc 0.75351452671\n",
      "Epoch: 15\n",
      "Epoch 14. Loss: 0.000697362095257, Train_acc 1.0, Test_acc 0.751640112465\n",
      "Epoch: 16\n",
      "Epoch 15. Loss: 0.000616264179419, Train_acc 1.0, Test_acc 0.752577319588\n",
      "Epoch: 17\n",
      "Epoch 16. Loss: 0.000552176198242, Train_acc 1.0, Test_acc 0.75351452671\n",
      "Epoch: 18\n",
      "Epoch 17. Loss: 0.000498155412061, Train_acc 1.0, Test_acc 0.751640112465\n",
      "Epoch: 19\n",
      "Epoch 18. Loss: 0.00045249474183, Train_acc 1.0, Test_acc 0.751640112465\n",
      "Epoch: 20\n",
      "Epoch 19. Loss: 0.000416519682526, Train_acc 1.0, Test_acc 0.751640112465\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "start_time = time.time()\n",
    "config = Config()\n",
    "corpus = Corpus(pos_file, neg_file, vocab_file, config.dev_split, config.seq_length, config.vocab_size)\n",
    "print(corpus)\n",
    "config.vocab_size = len(corpus.words)\n",
    "\n",
    "print('Configuring CNN model...')\n",
    "model = CNN(config)\n",
    "model.collect_params().initialize()\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': 0.001})\n",
    "\n",
    "train_loader = gluon.data.DataLoader(MRDataset(corpus.x_train, corpus.y_train), batch_size=128, shuffle=False)\n",
    "test_loader = gluon.data.DataLoader(MRDataset(corpus.x_test, corpus.y_test), batch_size=128, shuffle=False)\n",
    "\n",
    "total_batch = 0\n",
    "total_loss = 0.0\n",
    "best_acc_val = 0.0\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    cumulative_loss = 0.0\n",
    "    print('Epoch:', epoch + 1)\n",
    "    # load the training data in batch\n",
    "    \n",
    "    for data, label in train_loader:\n",
    "        batch_len = len(data)\n",
    "        # print('cur_batch:', batch_len)\n",
    "        with autograd.record():\n",
    "            output = model(data)\n",
    "            losses = loss(output, label)\n",
    "        losses.backward()\n",
    "        trainer.step(batch_len)\n",
    "        cumulative_loss += nd.sum(losses).asscalar()\n",
    "    \n",
    "    test_accuracy = evaluate_accuracy(test_loader, model)\n",
    "    train_accuracy = evaluate_accuracy(train_loader, model)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (epoch, cumulative_loss/len(corpus.x_train), train_accuracy, test_accuracy))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    Train and evaluate the model with training and validation data.\n",
    "    \"\"\"\n",
    "    print('Loading data...')\n",
    "    start_time = time.time()\n",
    "    config = Config()\n",
    "    corpus = Corpus(pos_file, neg_file, vocab_file, config.dev_split, config.seq_length, config.vocab_size)\n",
    "    print(corpus)\n",
    "    config.vocab_size = len(corpus.words)\n",
    "\n",
    "    print('Configuring CNN model...')\n",
    "    model = CNN(config)\n",
    "\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(model.collect_params(), 'sgd', {'learning_rate': 0.01})\n",
    "    \n",
    "    total_batch = 0\n",
    "    total_loss = 0.0\n",
    "    best_acc_val = 0.0\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        # load the training data in batch\n",
    "        train_loader = io.NDArrayIter(data={'data': nd.array(corpus.x_train)}, \n",
    "                                      label={'label': nd.array(corpus.y_train)}, \n",
    "                                      batch_size=config.batch_size)\n",
    "        for batch in train_loader:\n",
    "            cur_batch = config.batch_size - batch.pad\n",
    "            with autograd.record():\n",
    "                output = model(batch.data[0])\n",
    "                loss = softmax_cross_entropy(output, batch.label[:cur_batch])\n",
    "            loss.backward()\n",
    "            trainer.step(cur_batch)\n",
    "            print(loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # forward computation\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.data[0]\n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # print out intermediate status\n",
    "                avg_loss = total_loss / config.print_per_batch\n",
    "                total_loss = 0.0\n",
    "\n",
    "                _, pred_train = torch.max(outputs.data, 1)\n",
    "                corrects = (pred_train == targets.data).sum()\n",
    "                acc_train = corrects / len(x_batch)\n",
    "                loss_val, acc_val, _ = evaluate(model, val_data)  # evaluate on val data\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                    # store the best validation result\n",
    "                    best_acc_val = acc_val\n",
    "                    improved_str = '*'\n",
    "                    torch.save(model.state_dict(), model_file)\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, avg_loss, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "            # back propagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    test(model, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
